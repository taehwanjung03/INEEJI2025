{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0d02f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import requests\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "# Suppress warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    import shap\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_error, r2_score, mean_absolute_percentage_error, mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "\n",
    "plt.rcParams['font.family'] ='Malgun Gothic'\n",
    "plt.rcParams['axes.unicode_minus'] =False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "151f5000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with more than half NaN dropped. Remaining rows: 2811\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV\n",
    "df = pd.read_csv('C:/Users/User/Downloads/Kyushu Datasheets/data_with_press_and_weather.csv', low_memory=False)\n",
    "\n",
    "# Drop rows with excessive missing values\n",
    "missing_count = df.isnull().sum().sum()\n",
    "if missing_count > 0:\n",
    "    threshold = df.shape[1] // 2\n",
    "    df = df.dropna(thresh=threshold + 1)\n",
    "    print(f\"Rows with more than half NaN dropped. Remaining rows: {len(df)}\")\n",
    "else:\n",
    "    print(\"No missing values found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9607886f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and filter '강번'\n",
    "df['강번'] = pd.to_numeric(df['강번'], errors='coerce')\n",
    "df = df.dropna(subset=['강번'])\n",
    "df = df[df['강번'].between(9000, 500000000)]\n",
    "df = df[df['수율'].between(60, 100)]\n",
    "\n",
    "df.to_csv('For_my_use_1.csv', index=False)\n",
    "\n",
    "# Make sure the date column is in datetime format\n",
    "df['날짜'] = pd.to_datetime(df['날짜'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95c7fd3c-6307-41e4-8676-1b9584322bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable\n",
    "target = 'CC_P'\n",
    "\n",
    "'''\n",
    "# Outlier removal\n",
    "if 'Precipitation (mm)' in df.columns:\n",
    "    initial_count_oxygen = len(df)\n",
    "    df = df[df['Precipitation (mm)'] <= 10]\n",
    "    removed_count_oxygen = initial_count_oxygen - len(df)\n",
    "    print(f\"Removed {removed_count_oxygen} rows where {'Precipitation (mm)'} > 10.\")\n",
    "else:\n",
    "    print(f\"Warning: '{'Precipitation (mm)'}' column not found.\")\n",
    "'''\n",
    "# chin_variables = [col for col in df.columns if '친' in col and '친단' not in col]\n",
    "# df['chin_total'] = df[chin_variables].sum(axis=1)\n",
    "\n",
    "scraps_col = [\n",
    "    '스크랩_S01_친',\n",
    "    '스크랩_S01_자',\n",
    "    # '스크랩_S01_손', # for GPR\n",
    "    'A부스러기　친',\n",
    "    'A부스러기 자',\n",
    "    'A부스러기 손',\n",
    "    # '친단　친', # for GPR\n",
    "    '친단　자',\n",
    "    '친단　손',\n",
    "    '와셔 친',\n",
    "    # '와셔 자', # for GPR\n",
    "    # '와셔 손', # for GPR\n",
    "    'B부스러기　친',\n",
    "    'B부스러기 자',\n",
    "    'B부스러기 손',\n",
    "    '선다라이 친',\n",
    "    # '선다라이 자', # for GPR\n",
    "    # '선다라이 손', # for GPR\n",
    "    # '류선 친', # for GPR\n",
    "    '류선 자',\n",
    "    # '류선 손', # for GPR\n",
    "    '시타마와리(자동차 부품으로 예상됨)　친',\n",
    "    # '시타마와리(자동차 부품으로 예상됨)　자', # for GPR\n",
    "    # '시타마와리(자동차 부품으로 예상됨)　손', # for GPR\n",
    "    '엔진 친',\n",
    "    # '엔진 자',\n",
    "    # '엔진 손',\n",
    "    '슈레더 친',\n",
    "    '슈레더 자',\n",
    "    '슈레더 손',\n",
    "    'C프레스 친',\n",
    "    'C프레스 자',\n",
    "    # 'C프레스 손', # for GPR\n",
    "    # '다라이가루 친', # for GPR\n",
    "    '다라이가루 자',\n",
    "    '다라이가루 손',\n",
    "    '강류 친',\n",
    "    # '강류 자', # for GPR\n",
    "    # '강류 손', # for GPR\n",
    "    '타이어 친',\n",
    "    # '타이어　자', # for GPR\n",
    "    # '타이어　손', # for GPR\n",
    "    '주물　친',\n",
    "    # '주물　자', # for GPR\n",
    "    # '주물　손', # for GPR\n",
    "    # '알루미더스트　친', # for GPR\n",
    "    # '알루미더스트　자', # for GPR\n",
    "    # '알루미더스트　손', # for GPR\n",
    "    # 'AB프레스　친', # for GPR\n",
    "    # 'AB프레스　자', # for GPR\n",
    "    # 'AB프레스　손', # for GPR\n",
    "    # 'Mn강　친', # for GPR\n",
    "    # 'Mn강　자', # for GPR\n",
    "    # 'Mn강　손', # for GPR\n",
    "    # '캔버서　친', # for GPR\n",
    "    # '캔버서　자', # for GPR\n",
    "    # '캔버서　손', # for GPR\n",
    "    # '페인트캔　친', # for GPR\n",
    "    # '페인트캔　자', # for GPR\n",
    "    # '페인트캔　손', # for GPR\n",
    "    'V프레스　친',\n",
    "    # 'V프레스　자', # for GPR\n",
    "    # 'V프레스　손', # for GPR\n",
    "    # '캔프레스　친', # for GPR\n",
    "    '캔프레스　자',\n",
    "    # '캔프레스　손', # for GPR\n",
    "    '빌렛부스러기　친',\n",
    "    # '빌렛부스러기 자', # for GPR\n",
    "    # '빌렛부스러기 손', # for GPR\n",
    "    # 'SC（레들）바탕쇠　친', # for GPR\n",
    "    'SC（레들）바탕쇠　자',\n",
    "    # 'SC（레들）바탕쇠　손', # for GPR\n",
    "    # 'EF바탕쇠　친', # for GPR\n",
    "    'EF바탕쇠　자',\n",
    "    # 'EF바탕쇠　손', # for GPR\n",
    "    # 'CC바탕쇠　친', # for GPR\n",
    "    # 'CC바탕쇠　자', # for GPR\n",
    "    # 'CC바탕쇠　손', # for GPR\n",
    "    '정비부스러기　친',\n",
    "    # '정비부스러기　자', # for GPR\n",
    "    # '정비부스러기　손', # for GPR\n",
    "]\n",
    "\n",
    "chin_variables = [col for col in df.columns if '친' in col and '친단' not in col]\n",
    "ja_variables = [col for col in df.columns if '자' in col]\n",
    "son_variables = [col for col in df.columns if '손' in col]\n",
    "\n",
    "scraps = chin_variables + ja_variables + son_variables\n",
    "\n",
    "# variables = []\n",
    "X = df[scraps_col + ['MD_C', 'MD_Mn', 'MD_Cr', 'MD_Si', 'MD_S', '장입량t', \n",
    "                     'TAP-TAP', '온도', 'CaO', '사용전력량', 'O2사용량_합계'\n",
    "                     ]]\n",
    "y = df[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60b1a49e-20f0-4aa4-ab39-3ee783360ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature pipeline\n",
    "# 훈련 데이터의 강종별 'C-inj(佐山)' 평균과 표준편차를 저장함.\n",
    "# 이후 for 문을 돌면서 현재 샘플의 강종명을 저장해두고,\n",
    "# 강종명이 일치하고, 정합성을 만족하는 이전 시점 샘플 탐색.\n",
    "# 탐색에 성공한 경우 해당 샘플의 'C-inj(佐山)'을 불러와서 현재 샘플 값으로 사용함.\n",
    "# 탐색에 실패한 경우 저장된 훈련 데이터의 강종별 평균을 사용함.\n",
    "# 평균값도 없으면 해당 인스턴스의 'C-inj(佐山)'은 NaN 값으로 채움.\n",
    "\n",
    "# 정합성 만족 기준은 평균 +- n 표준편차 이내\n",
    "\n",
    "def feature_pipeline(df, additives_col, etc_col, train_df_for_stats=None, time_col='time', n_std=3, drop_na=True):\n",
    "    \"\"\"\n",
    "    Performs feature engineering on a given dataframe.\n",
    "    - If train_df_for_stats is provided (inference mode), it uses stats from that dataframe.\n",
    "    - If not (training mode), it calculates stats from the input df itself.\n",
    "\n",
    "    The logic is as follows:\n",
    "    1. Calculate mean and std for each steel grade from the stats source.\n",
    "    2. For each row in the target df, find the value from the previous heat of the same grade.\n",
    "    3. If the previous value is valid (within n_std of the grade's mean), use it.\n",
    "    4. If not, use the grade's average as a fallback.\n",
    "    5. Drop rows with remaining NaNs if drop_na is True.\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original DataFrame\n",
    "    df_processed = df.copy()\n",
    "\n",
    "    # --- 1. Determine the source for statistics and calculate them ---\n",
    "    if train_df_for_stats is not None:\n",
    "        # Inference mode: Use the provided training data to calculate stats\n",
    "        stats_source_df = train_df_for_stats\n",
    "        print(\"Running in INFERENCE mode: Using stats from provided train_df.\")\n",
    "    else:\n",
    "        # Training mode: Use the input dataframe itself to calculate stats\n",
    "        stats_source_df = df_processed\n",
    "        print(\"Running in TRAINING mode: Using stats from the input df itself.\")\n",
    "        \n",
    "    # Calculate stats for the validity check (mean and std from the stats source)\n",
    "    grade_stats = stats_source_df.groupby('강종명')[additives_col].agg(['mean', 'std'])\n",
    "\n",
    "    # --- 2. Apply feature engineering to the target dataset ---\n",
    "    # Sort the target dataframe by time to correctly identify the previous heat\n",
    "    df_processed.sort_values(by=time_col, inplace=True)\n",
    "\n",
    "    # Loop through each additive column\n",
    "    for col in additives_col:\n",
    "        # Get the value from the previous heat of the same grade\n",
    "        prev_values = df_processed.groupby('강종명')[col].shift(1)\n",
    "\n",
    "        # Get the corresponding stats for each row's grade, using the stats from the source\n",
    "        mean_values = df_processed['강종명'].map(grade_stats[col]['mean'])\n",
    "        std_values = df_processed['강종명'].map(grade_stats[col]['std'])\n",
    "\n",
    "        # --- Apply the validation logic ---\n",
    "        # Default to using the grade's average value\n",
    "        new_col = mean_values.copy()\n",
    "\n",
    "        # Calculate validity bounds using the source stats\n",
    "        lower_bound = mean_values - n_std * std_values\n",
    "        upper_bound = mean_values + n_std * std_values\n",
    "\n",
    "        # Create a mask for rows where the previous value is valid\n",
    "        is_valid_prev = (prev_values.notna()) & \\\n",
    "                        (prev_values >= lower_bound) & \\\n",
    "                        (prev_values <= upper_bound)\n",
    "\n",
    "        # --- Print count of times where mean is used ---\n",
    "        mean_used_mask = ~is_valid_prev\n",
    "        count_mean_used = mean_used_mask.sum()\n",
    "        if count_mean_used > 0:\n",
    "            print(f\"  - Using mean for '{col}' {count_mean_used} times.\")\n",
    "\n",
    "        # Where the mask is True, update the new column with the valid previous value\n",
    "        new_col.loc[is_valid_prev] = prev_values[is_valid_prev]\n",
    "        \n",
    "        # Replace the original column with the newly calculated values\n",
    "        df_processed[col] = new_col\n",
    "\n",
    "    # 3. If drop_na is True, remove rows that couldn't be filled\n",
    "    if drop_na:\n",
    "        initial_rows = len(df_processed)\n",
    "        df_processed.dropna(subset=additives_col, inplace=True)\n",
    "        print(f'  - NaN instances dropped: {initial_rows - len(df_processed)}')\n",
    "\n",
    "    # 4. Drop etc_col as specified\n",
    "    df_final = df_processed.drop(columns=etc_col)\n",
    "    \n",
    "    return df_final, df_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac58abce-98fe-43c4-91e6-4dc559710d89",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'selected_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m test_start_date = \u001b[33m'\u001b[39m\u001b[33m2024-10-01\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# 3. Filter the DataFrame to create the training set\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m train_df = \u001b[43mselected_df\u001b[49m[selected_df[\u001b[33m'\u001b[39m\u001b[33mtime\u001b[39m\u001b[33m'\u001b[39m] < test_start_date]\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# 4. Filter the DataFrame to create the test set\u001b[39;00m\n\u001b[32m      9\u001b[39m test_df = selected_df[selected_df[\u001b[33m'\u001b[39m\u001b[33mtime\u001b[39m\u001b[33m'\u001b[39m] >= test_start_date]\n",
      "\u001b[31mNameError\u001b[39m: name 'selected_df' is not defined"
     ]
    }
   ],
   "source": [
    "# 훈련/테스트 분할.\n",
    "# 훈련 데이터는 2024년 1월~9월, 테스트 데이터는 2024년 10월~12월 데이터로 설정함.\n",
    "test_start_date = '2024-10-01'\n",
    "\n",
    "# 3. Filter the DataFrame to create the training set\n",
    "train_df = selected_df[selected_df['time'] < test_start_date]\n",
    "\n",
    "# 4. Filter the DataFrame to create the test set\n",
    "test_df = selected_df[selected_df['time'] >= test_start_date]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66df0af8-473e-480a-945e-846c9f70bebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.to_csv('temp_train_df_before.csv', encoding='utf-8-sig')\n",
    "# test_df.to_csv('temp_test_df_before.csv', encoding='utf-8-sig')\n",
    "\n",
    "# --- 시나리오 1: 모델 학습 (Training) ---\n",
    "# train_data만 함수에 전달합니다.\n",
    "# 이 경우, 함수는 train_data 자체의 통계치(평균, 표준편차)를 계산하여 피처 엔지니어링을 수행합니다.\n",
    "print(\"--- PROCESSING TRAINING DATA ---\")\n",
    "processed_train_df, processed_train_df_with_etc_col = feature_pipeline(\n",
    "    df=train_df,\n",
    "    additives_col=additives_col,\n",
    "    etc_col=etc_col,\n",
    "    )\n",
    "\n",
    "# --- 시나리오 2: 모델 추론 (Inference) ---\n",
    "# test_data를 처리하기 위해 함수를 호출하지만, 통계치 계산의 기준이 될 train_data를 함께 전달합니다.\n",
    "print(\"--- PROCESSING TEST DATA ---\")\n",
    "processed_test_df, processed_test_df_with_etc_col = feature_pipeline(\n",
    "    df=test_df, # df: 모델의 학습 데이터 이후 시점의 데이터 (즉, 학습 데이터 마지막 시점 ~ 현재 추론 시점까지의 데이터)\n",
    "    additives_col=additives_col,\n",
    "    etc_col=etc_col,\n",
    "    train_df_for_stats=train_df, # 모델의 학습 데이터\n",
    "    )\n",
    "\n",
    "# processed_train_df_with_etc_col.to_csv('temp_train_df_after.csv', encoding='utf-8-sig')\n",
    "# processed_test_df_with_etc_col.to_csv('temp_test_df_after.csv', encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50fbba3-3fc0-4f99-916a-c9e528cf6ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋에서 time, target 컬럼 제거\n",
    "X_train = processed_train_df.drop(columns=time_col + target_col)\n",
    "X_test = processed_test_df.drop(columns=time_col + target_col)\n",
    "\n",
    "y_train = processed_train_df[target_col]\n",
    "y_test = processed_test_df[target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42793dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model training and evaluation functions ---\n",
    "\n",
    "def evaluate_model(name, model, X_train, y_train, X_test, y_test, df):\n",
    "    print(f\"\\n--- {name} Model Evaluation ---\")\n",
    "    train_pred = model.predict(X_train)\n",
    "    test_pred = model.predict(X_test)\n",
    "    # --- Calculate Metrics ---\n",
    "    # Train\n",
    "    mse_train = mean_squared_error(y_train, train_pred)\n",
    "    rmse_train = np.sqrt(mse_train)\n",
    "    mae_train = mean_absolute_error(y_train, train_pred)\n",
    "    mape_train = np.mean(np.abs((y_train - train_pred) / y_train)) * 100\n",
    "    r2_train = r2_score(y_train, train_pred)\n",
    "\n",
    "    # Test\n",
    "    mse_test = mean_squared_error(y_test, test_pred)\n",
    "    rmse_test = np.sqrt(mse_test)\n",
    "    mae_test = mean_absolute_error(y_test, test_pred)\n",
    "    mape_test = np.mean(np.abs((y_test - test_pred) / y_test)) * 100\n",
    "    r2_test = r2_score(y_test, test_pred)\n",
    "\n",
    "    # --- Print Metrics ---\n",
    "    print(f\"{name} Train Scores:\")\n",
    "    print(f\"  MSE:  {mse_train:.2f}\")\n",
    "    print(f\"  RMSE: {rmse_train:.2f}\")\n",
    "    print(f\"  MAE:  {mae_train:.2f}\")\n",
    "    print(f\"  MAPE: {mape_train:.2f}%\")\n",
    "    print(f\"  R²:   {r2_train:.3f}\")\n",
    "\n",
    "    print(f\"\\n{name} Test Scores:\")\n",
    "    print(f\"  MSE:  {mse_test:.2f}\")\n",
    "    print(f\"  RMSE: {rmse_test:.2f}\")\n",
    "    print(f\"  MAE:  {mae_test:.2f}\")\n",
    "    print(f\"  MAPE: {mape_test:.2f}%\")\n",
    "    print(f\"  R²:   {r2_test:.3f}\")\n",
    "    \n",
    "    # Plot\n",
    "    train_df = df[['강번']].loc[X_train.index].copy()\n",
    "    train_df['Actual'] = y_train.values\n",
    "    train_df['Predicted'] = train_pred\n",
    "    train_df['Set'] = 'Train'\n",
    "\n",
    "    test_df = df[['강번']].loc[X_test.index].copy()\n",
    "    test_df['Actual'] = y_test.values\n",
    "    test_df['Predicted'] = test_pred\n",
    "    test_df['Set'] = 'Test'\n",
    "\n",
    "    combined_df = pd.concat([train_df, test_df]).sort_values('강번').reset_index(drop=True)\n",
    "    combined_df['Time Index'] = range(len(combined_df))\n",
    "\n",
    "    plt.figure(figsize=(16, 4))\n",
    "    plt.plot(combined_df['Time Index'], combined_df['Actual'], label='Actual', linewidth=2)\n",
    "    plt.plot(combined_df['Time Index'], combined_df['Predicted'], label='Predicted', linestyle='--', linewidth=2)\n",
    "    test_start = combined_df[combined_df['Set'] == 'Test']['Time Index'].min()\n",
    "    plt.axvline(x=test_start, color='red', linestyle=':', label='Test Start')\n",
    "    plt.xlabel('Time Index (chronological)')\n",
    "    plt.ylabel(target)\n",
    "    plt.title(f'Actual vs Predicted {target} ({name}) - R² = {r2_test:.3f}, RMSE = {rmse_test:.1f}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Test-only plot\n",
    "    test_df = test_df.sort_values('강번').reset_index(drop=True)\n",
    "    test_df['Time Index'] = range(len(test_df))\n",
    "\n",
    "    plt.figure(figsize=(18, 3))\n",
    "    plt.plot(test_df['Time Index'], test_df['Actual'], label='Actual', linewidth=2)\n",
    "    plt.plot(test_df['Time Index'], test_df['Predicted'], label='Predicted', linestyle='--', linewidth=2)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98c4fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Linear Regression ---\n",
    "lin_model = LinearRegression()\n",
    "lin_model.fit(X_train, y_train)\n",
    "evaluate_model(\"Linear Regression\", lin_model, X_train, y_train, X_test, y_test, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31afae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- XGBoost ---\n",
    "xgb_model = XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.005,\n",
    "    max_depth=4,\n",
    "    random_state=42\n",
    ")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "evaluate_model(\"XGBoost\", xgb_model, X_train, y_train, X_test, y_test, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0ae9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Random Forest ---\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=None,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_model.fit(X_train, y_train)\n",
    "evaluate_model(\"Random Forest\", rf_model, X_train, y_train, X_test, y_test, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d824323",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78695343",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Added on 2025-08-08 04:07:56: safer evaluator for SVM/ANN (works with 1D/2D preds) ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "def evaluate_model(name, model, X_train, y_train, X_test, y_test, df):\n",
    "    print(f\"\\n--- {name} Model Evaluation ---\")\n",
    "    train_pred = model.predict(X_train)\n",
    "    test_pred = model.predict(X_test)\n",
    "    # Ensure 1-D arrays\n",
    "    train_pred = np.ravel(train_pred)\n",
    "    test_pred = np.ravel(test_pred)\n",
    "\n",
    "    # Convert y to 1-D numpy arrays (handles Series/DataFrame/ndarray)\n",
    "    ytr = np.ravel(np.array(y_train))\n",
    "    yte = np.ravel(np.array(y_test))\n",
    "\n",
    "    # --- Metrics ---\n",
    "    mse_train = mean_squared_error(ytr, train_pred)\n",
    "    rmse_train = np.sqrt(mse_train)\n",
    "    mae_train = mean_absolute_error(ytr, train_pred)\n",
    "    # Safe MAPE\n",
    "    denom_tr = np.where(ytr == 0, np.finfo(float).eps, ytr)\n",
    "    mape_train = np.mean(np.abs((ytr - train_pred) / denom_tr)) * 100\n",
    "    r2_train = r2_score(ytr, train_pred)\n",
    "\n",
    "    mse_test = mean_squared_error(yte, test_pred)\n",
    "    rmse_test = np.sqrt(mse_test)\n",
    "    mae_test = mean_absolute_error(yte, test_pred)\n",
    "    denom_te = np.where(yte == 0, np.finfo(float).eps, yte)\n",
    "    mape_test = np.mean(np.abs((yte - test_pred) / denom_te)) * 100\n",
    "    r2_test = r2_score(yte, test_pred)\n",
    "\n",
    "    # --- Print ---\n",
    "    print(f\"{name} Train Scores:\")\n",
    "    print(f\"  MSE:  {mse_train:.2f}\")\n",
    "    print(f\"  RMSE: {rmse_train:.2f}\")\n",
    "    print(f\"  MAE:  {mae_train:.2f}\")\n",
    "    print(f\"  MAPE: {mape_train:.2f}%\")\n",
    "    print(f\"  R²:   {r2_train:.3f}\")\n",
    "\n",
    "    print(f\"\\n{name} Test Scores:\")\n",
    "    print(f\"  MSE:  {mse_test:.2f}\")\n",
    "    print(f\"  RMSE: {rmse_test:.2f}\")\n",
    "    print(f\"  MAE:  {mae_test:.2f}\")\n",
    "    print(f\"  MAPE: {mape_test:.2f}%\")\n",
    "    print(f\"  R²:   {r2_test:.3f}\")\n",
    "\n",
    "    # --- Plots ---\n",
    "    # Assumes df has a '강번' column and X indices align with df\n",
    "    train_df = df[['강번']].loc[X_train.index].copy()\n",
    "    train_df['Actual'] = ytr\n",
    "    train_df['Predicted'] = train_pred\n",
    "    train_df['Set'] = 'Train'\n",
    "\n",
    "    test_df = df[['강번']].loc[X_test.index].copy()\n",
    "    test_df['Actual'] = yte\n",
    "    test_df['Predicted'] = test_pred\n",
    "    test_df['Set'] = 'Test'\n",
    "\n",
    "    combined_df = pd.concat([train_df, test_df]).sort_values('강번').reset_index(drop=True)\n",
    "    combined_df['Time Index'] = range(len(combined_df))\n",
    "\n",
    "    plt.figure(figsize=(16, 4))\n",
    "    plt.plot(combined_df['Time Index'], combined_df['Actual'], label='Actual', linewidth=2)\n",
    "    plt.plot(combined_df['Time Index'], combined_df['Predicted'], label='Predicted', linestyle='--', linewidth=2)\n",
    "    test_start = combined_df[combined_df['Set'] == 'Test']['Time Index'].min()\n",
    "    if pd.notnull(test_start):\n",
    "        plt.axvline(x=test_start, color='red', linestyle=':', label='Test Start')\n",
    "    plt.xlabel('Time Index (chronological)')\n",
    "    plt.ylabel(target)\n",
    "    plt.title(f'Actual vs Predicted {target} ({name}) - R² = {r2_test:.3f}, RMSE = {rmse_test:.1f}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Test-only plot\n",
    "    test_df = test_df.sort_values('강번').reset_index(drop=True)\n",
    "    test_df['Time Index'] = range(len(test_df))\n",
    "\n",
    "    plt.figure(figsize=(18, 3))\n",
    "    plt.plot(test_df['Time Index'], test_df['Actual'], label='Actual', linewidth=2)\n",
    "    plt.plot(test_df['Time Index'], test_df['Predicted'], label='Predicted', linestyle='--', linewidth=2)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a97c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Added on 2025-08-08 04:07:56: SVM (SVR) and ANN (MLPRegressor) implementations ===\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# SVR pipeline (scale features)\n",
    "svr_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svr', SVR(kernel='rbf', C=10.0, epsilon=0.1, gamma='scale'))\n",
    "])\n",
    "\n",
    "svr_pipeline.fit(X_train, y_train)\n",
    "svr_test_df = evaluate_model(\"SVR (RBF)\", svr_pipeline, X_train, y_train, X_test, y_test, df)\n",
    "\n",
    "# ANN pipeline (scale features)\n",
    "ann_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('mlp', MLPRegressor(hidden_layer_sizes=(128, 64),\n",
    "                         activation='relu',\n",
    "                         solver='adam',\n",
    "                         learning_rate='adaptive',\n",
    "                         max_iter=1000,\n",
    "                         early_stopping=True,\n",
    "                         random_state=42))\n",
    "])\n",
    "\n",
    "ann_pipeline.fit(X_train, y_train)\n",
    "ann_test_df = evaluate_model(\"ANN (MLPRegressor)\", ann_pipeline, X_train, y_train, X_test, y_test, df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
